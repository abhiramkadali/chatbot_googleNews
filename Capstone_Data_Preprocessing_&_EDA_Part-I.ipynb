{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\akadali\\\\Desktop\\\\Deep_NLP\\\\MLG_Capstone_ChatBot\\\\ChatBot_GoogleW2V'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\akadali\\\\Desktop\\\\Deep_NLP\\\\MLG_Capstone_ChatBot\\\\ChatBot_GoogleW2V\\\\raw_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Referral Emails.CSV', encoding = 'latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Email Data Cleanup Steps\n",
    "-----------\n",
    "Below steps are performed on EMail dataset to preprocess and extract body texts from the email. This dataset is acquired by exporting emails from the last 3 years into a 'csv' file.\n",
    "* Drop the columns which are of no use\n",
    "* Drop unnecessary columns\n",
    "* Remove Emails with empty body text - empty subject\n",
    "* Remove Automatic replies\n",
    "* Remove undeliverable emails\n",
    "* Remove all 'na' values\n",
    "* Remove emails with Subject has 'Validation' and 'Yammer'\n",
    "* Remove LCSP Approval emails\n",
    "* Remove Emails with Subjet has 'Bot' in it...as these are all bot status emails and are of no use for us\n",
    "* Remove Referral status Communications - These are the communication sent out to employees and in return, they ask questions. For now these can be removed as we are looking for first contact queries\n",
    "* Looking for most frequent Emails subjects. If a particular subject is appearing too many times, probably that consists of long chain of emails and fo no good to our cause. Hence remove the emails with that subject\n",
    "* Remove Emails sent from US Talent Referral Program and 'Replied Emails'. As these are the response sent by the Talent Referral team and we do not need them for now, as we are trying to work on questions here.\n",
    "* Dropping emails with Subjects start with 'RE' and 'Re' as we need only FCQs (First Contact Queries)\n",
    "* Check if there are any emails with subject 'Action Required'. These are the validation emails sent out to recruiters by the team\n",
    "* Look at the Most frequent statuses again adn remove if there are any\n",
    "* Removing all rows/records that are not sent to US Talent Referral Program mailbox\n",
    "* Removing emails received from \"Callahan, Jane\", \"Tiffany, Mercer\", \"US CTS Talent R&A\", 'Deloitte (O365D) on Yammer'm \"YourOnlineProfessor\", \"US Talent CIC Inbox\", \"US R10 Referral Response\"\n",
    "* Drop the remaining unnecessary columns\n",
    "* Removing Signatures from the emails\n",
    "* Calculating 'Verbs count', 'Line Count', 'Total Word Count' and 'Question Length' for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Drop the columns which are of no use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['BCC: (Name)', 'BCC: (Address)', 'BCC: (Type)', 'Billing Information', 'Categories', 'Importance', 'Mileage', 'Sensitivity'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27872, 11)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This dataset has about 27872 emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A definition to look at the quick descriptive statistics of the dataset\n",
    "def quick_analysis(data):\n",
    "    analysis = pd.DataFrame({\"Data Types\": data.dtypes, \n",
    "                             \"Null Values\": data.apply(lambda x:sum(x.isnull())),\n",
    "                             \"NaN Values\": data.isna().sum(),\n",
    "                             \"Count\": data.count(),\n",
    "                             #\"Mean\": data.mean().round(2),\n",
    "                             #\"First Quantile\":data.quantile(0.25).round(2),\n",
    "                             #\"Median or Q2\":data.median().round(2),\n",
    "                             #\"Thrid Quantile\":data.quantile(0.75).round(2),\n",
    "                             #\"IQR\":(data.quantile(0.75)-data.quantile(0.25)).round(2),\n",
    "                             #\"No. of outliers\":data[((data > data.quantile(0.75) + 1.5*((data.quantile(0.75)-data.quantile(0.25)).round(2))) | (data < data.quantile(0.25).round(2)-1.5*(data.quantile(0.75)-data.quantile(0.25)).round(2)))].count(),\n",
    "                             #\"Outliers above UQR\":data[(data > data.quantile(0.75) + 1.5*(data.quantile(0.75)-data.quantile(0.25)).round(2))].count(),\n",
    "                             \"Unique Count\": data.nunique()})\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['CC: (Name)','CC: (Address)', 'CC: (Type)'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick_analysis(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remove Emails with empty body text - empty subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Body</th>\n",
       "      <th>From: (Name)</th>\n",
       "      <th>From: (Address)</th>\n",
       "      <th>From: (Type)</th>\n",
       "      <th>To: (Name)</th>\n",
       "      <th>To: (Address)</th>\n",
       "      <th>To: (Type)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Automatic reply: When we have great talent, yo...</td>\n",
       "      <td>Hello,\\r\\n\\r\\n \\r\\n\\r\\nThank you for your mess...</td>\n",
       "      <td>Blanchard, John</td>\n",
       "      <td>/O=EXCHANGELABS/OU=EXCHANGE ADMINISTRATIVE GRO...</td>\n",
       "      <td>EX</td>\n",
       "      <td>US Talent Referral Program</td>\n",
       "      <td>/o=ExchangeLabs/ou=Exchange Administrative Gro...</td>\n",
       "      <td>EX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Automatic reply: When we have great talent, yo...</td>\n",
       "      <td>**Automatic reply**\\r\\n\\r\\n \\r\\n\\r\\nDear Sende...</td>\n",
       "      <td>Rocolcol, Lady Rose</td>\n",
       "      <td>/O=EXCHANGELABS/OU=EXCHANGE ADMINISTRATIVE GRO...</td>\n",
       "      <td>EX</td>\n",
       "      <td>US Talent Referral Program</td>\n",
       "      <td>/o=ExchangeLabs/ou=Exchange Administrative Gro...</td>\n",
       "      <td>EX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Automatic reply: When we have great talent, yo...</td>\n",
       "      <td>Hi, I am currently out of the office and will ...</td>\n",
       "      <td>Altman, Bradley</td>\n",
       "      <td>/O=EXCHANGELABS/OU=EXCHANGE ADMINISTRATIVE GRO...</td>\n",
       "      <td>EX</td>\n",
       "      <td>US Talent Referral Program</td>\n",
       "      <td>/o=ExchangeLabs/ou=Exchange Administrative Gro...</td>\n",
       "      <td>EX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Automatic reply: When we have great talent, yo...</td>\n",
       "      <td>I am currently out of the office, returning Tu...</td>\n",
       "      <td>Fonseca, Mary Clare</td>\n",
       "      <td>/O=EXCHANGELABS/OU=EXCHANGE ADMINISTRATIVE GRO...</td>\n",
       "      <td>EX</td>\n",
       "      <td>US Talent Referral Program</td>\n",
       "      <td>/o=ExchangeLabs/ou=Exchange Administrative Gro...</td>\n",
       "      <td>EX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Automatic reply: When we have great talent, yo...</td>\n",
       "      <td>Thank you for your email. I will be out of the...</td>\n",
       "      <td>Vasquez, Aimee</td>\n",
       "      <td>/O=EXCHANGELABS/OU=EXCHANGE ADMINISTRATIVE GRO...</td>\n",
       "      <td>EX</td>\n",
       "      <td>US Talent Referral Program</td>\n",
       "      <td>/o=ExchangeLabs/ou=Exchange Administrative Gro...</td>\n",
       "      <td>EX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27867</td>\n",
       "      <td>RE: Referral Submission Resume Issue  -10111410</td>\n",
       "      <td>Hi Christopher,\\r\\n\\r\\n \\r\\n\\r\\nThank you for ...</td>\n",
       "      <td>US Talent Referral Program</td>\n",
       "      <td>/O=EXCHANGELABS/OU=EXCHANGE ADMINISTRATIVE GRO...</td>\n",
       "      <td>EX</td>\n",
       "      <td>Hamilton, Christopher</td>\n",
       "      <td>/o=ExchangeLabs/ou=Exchange Administrative Gro...</td>\n",
       "      <td>EX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27868</td>\n",
       "      <td>RE: Referral Source Question 25888696</td>\n",
       "      <td>Hi Aaron,\\r\\n\\r\\n \\r\\n\\r\\nHope you are doing w...</td>\n",
       "      <td>US Talent Referral Program</td>\n",
       "      <td>/O=EXCHANGELABS/OU=EXCHANGE ADMINISTRATIVE GRO...</td>\n",
       "      <td>EX</td>\n",
       "      <td>Fleischmann, Aaron</td>\n",
       "      <td>/o=ExchangeLabs/ou=Exchange Administrative Gro...</td>\n",
       "      <td>EX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27869</td>\n",
       "      <td>RE: Referral Source Question 25888696</td>\n",
       "      <td>Hi Angela,\\r\\n\\r\\n \\r\\n\\r\\nThank you for the u...</td>\n",
       "      <td>US Talent Referral Program</td>\n",
       "      <td>/O=EXCHANGELABS/OU=EXCHANGE ADMINISTRATIVE GRO...</td>\n",
       "      <td>EX</td>\n",
       "      <td>Goss, Angela</td>\n",
       "      <td>/o=ExchangeLabs/ou=Exchange Administrative Gro...</td>\n",
       "      <td>EX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27870</td>\n",
       "      <td>RE: Non-Target School Campus Recruiting</td>\n",
       "      <td>Hi Lucas,\\r\\n\\r\\n \\r\\n\\r\\nThank you for reachi...</td>\n",
       "      <td>US Talent Referral Program</td>\n",
       "      <td>/O=EXCHANGELABS/OU=EXCHANGE ADMINISTRATIVE GRO...</td>\n",
       "      <td>EX</td>\n",
       "      <td>Booth, Lucas</td>\n",
       "      <td>/o=ExchangeLabs/ou=Exchange Administrative Gro...</td>\n",
       "      <td>EX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27871</td>\n",
       "      <td>RE: Sittambalam, Jason: Referral for Colleague</td>\n",
       "      <td>Hi Jason,\\r\\n\\r\\n \\r\\n\\r\\nThank you for reachi...</td>\n",
       "      <td>US Talent Referral Program</td>\n",
       "      <td>/O=EXCHANGELABS/OU=EXCHANGE ADMINISTRATIVE GRO...</td>\n",
       "      <td>EX</td>\n",
       "      <td>Sittambalam, Jason</td>\n",
       "      <td>/o=ExchangeLabs/ou=Exchange Administrative Gro...</td>\n",
       "      <td>EX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27704 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Subject  \\\n",
       "0      Automatic reply: When we have great talent, yo...   \n",
       "1      Automatic reply: When we have great talent, yo...   \n",
       "2      Automatic reply: When we have great talent, yo...   \n",
       "3      Automatic reply: When we have great talent, yo...   \n",
       "4      Automatic reply: When we have great talent, yo...   \n",
       "...                                                  ...   \n",
       "27867    RE: Referral Submission Resume Issue  -10111410   \n",
       "27868              RE: Referral Source Question 25888696   \n",
       "27869              RE: Referral Source Question 25888696   \n",
       "27870            RE: Non-Target School Campus Recruiting   \n",
       "27871    RE: Sittambalam, Jason: Referral for Colleague    \n",
       "\n",
       "                                                    Body  \\\n",
       "0      Hello,\\r\\n\\r\\n \\r\\n\\r\\nThank you for your mess...   \n",
       "1      **Automatic reply**\\r\\n\\r\\n \\r\\n\\r\\nDear Sende...   \n",
       "2      Hi, I am currently out of the office and will ...   \n",
       "3      I am currently out of the office, returning Tu...   \n",
       "4      Thank you for your email. I will be out of the...   \n",
       "...                                                  ...   \n",
       "27867  Hi Christopher,\\r\\n\\r\\n \\r\\n\\r\\nThank you for ...   \n",
       "27868  Hi Aaron,\\r\\n\\r\\n \\r\\n\\r\\nHope you are doing w...   \n",
       "27869  Hi Angela,\\r\\n\\r\\n \\r\\n\\r\\nThank you for the u...   \n",
       "27870  Hi Lucas,\\r\\n\\r\\n \\r\\n\\r\\nThank you for reachi...   \n",
       "27871  Hi Jason,\\r\\n\\r\\n \\r\\n\\r\\nThank you for reachi...   \n",
       "\n",
       "                     From: (Name)  \\\n",
       "0                 Blanchard, John   \n",
       "1             Rocolcol, Lady Rose   \n",
       "2                 Altman, Bradley   \n",
       "3             Fonseca, Mary Clare   \n",
       "4                  Vasquez, Aimee   \n",
       "...                           ...   \n",
       "27867  US Talent Referral Program   \n",
       "27868  US Talent Referral Program   \n",
       "27869  US Talent Referral Program   \n",
       "27870  US Talent Referral Program   \n",
       "27871  US Talent Referral Program   \n",
       "\n",
       "                                         From: (Address) From: (Type)  \\\n",
       "0      /O=EXCHANGELABS/OU=EXCHANGE ADMINISTRATIVE GRO...           EX   \n",
       "1      /O=EXCHANGELABS/OU=EXCHANGE ADMINISTRATIVE GRO...           EX   \n",
       "2      /O=EXCHANGELABS/OU=EXCHANGE ADMINISTRATIVE GRO...           EX   \n",
       "3      /O=EXCHANGELABS/OU=EXCHANGE ADMINISTRATIVE GRO...           EX   \n",
       "4      /O=EXCHANGELABS/OU=EXCHANGE ADMINISTRATIVE GRO...           EX   \n",
       "...                                                  ...          ...   \n",
       "27867  /O=EXCHANGELABS/OU=EXCHANGE ADMINISTRATIVE GRO...           EX   \n",
       "27868  /O=EXCHANGELABS/OU=EXCHANGE ADMINISTRATIVE GRO...           EX   \n",
       "27869  /O=EXCHANGELABS/OU=EXCHANGE ADMINISTRATIVE GRO...           EX   \n",
       "27870  /O=EXCHANGELABS/OU=EXCHANGE ADMINISTRATIVE GRO...           EX   \n",
       "27871  /O=EXCHANGELABS/OU=EXCHANGE ADMINISTRATIVE GRO...           EX   \n",
       "\n",
       "                       To: (Name)  \\\n",
       "0      US Talent Referral Program   \n",
       "1      US Talent Referral Program   \n",
       "2      US Talent Referral Program   \n",
       "3      US Talent Referral Program   \n",
       "4      US Talent Referral Program   \n",
       "...                           ...   \n",
       "27867       Hamilton, Christopher   \n",
       "27868          Fleischmann, Aaron   \n",
       "27869                Goss, Angela   \n",
       "27870                Booth, Lucas   \n",
       "27871          Sittambalam, Jason   \n",
       "\n",
       "                                           To: (Address) To: (Type)  \n",
       "0      /o=ExchangeLabs/ou=Exchange Administrative Gro...         EX  \n",
       "1      /o=ExchangeLabs/ou=Exchange Administrative Gro...         EX  \n",
       "2      /o=ExchangeLabs/ou=Exchange Administrative Gro...         EX  \n",
       "3      /o=ExchangeLabs/ou=Exchange Administrative Gro...         EX  \n",
       "4      /o=ExchangeLabs/ou=Exchange Administrative Gro...         EX  \n",
       "...                                                  ...        ...  \n",
       "27867  /o=ExchangeLabs/ou=Exchange Administrative Gro...         EX  \n",
       "27868  /o=ExchangeLabs/ou=Exchange Administrative Gro...         EX  \n",
       "27869  /o=ExchangeLabs/ou=Exchange Administrative Gro...         EX  \n",
       "27870  /o=ExchangeLabs/ou=Exchange Administrative Gro...         EX  \n",
       "27871  /o=ExchangeLabs/ou=Exchange Administrative Gro...         EX  \n",
       "\n",
       "[27704 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remove Automatic replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(data['Subject'].str.contains('Automatic reply'))\n",
    "#data[data['Subject'].str.contains('Automatic reply', na = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data[data['Subject'].str.contains('Automatic reply', na = False)].index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remove undeliverable emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['Subject'].str.contains(\"Undeliverable\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data[data['Subject'].str.contains(\"Undeliverable\", na = False)].index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remove all na values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remove emails with Subject has 'Validation' and 'Yammer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['Subject'].str.contains(\"Validation\").sum()\n",
    "data.drop(data[data['Subject'].str.contains(\"Validation\", na = False)].index, inplace = True)\n",
    "data.drop(data[data['Subject'].str.contains(\"Yammer\", na = False)].index, inplace = True)\n",
    "data.drop(data[data['Subject'].str.contains(\"validation\", na = False)].index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remove LCSP Approval emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['Subject'].str.contains(\"LCSP\").sum()\n",
    "data.drop(data[data['Subject'].str.contains(\"LCSP\", na = False)].index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remove Emails with Subjet has 'Bot' in it...as these are all bot status emails and are of no use for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['Subject'].str.contains(\"Bot\").sum()\n",
    "data.drop(data[data['Subject'].str.contains(\"Bot\", na = False)].index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remove Referral status Communications - These are the communication sent out to employees and in return, they ask questions. For now these can be removed as we are looking for first contact queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "994\n",
      "8\n",
      "14\n",
      "2604\n"
     ]
    }
   ],
   "source": [
    "print(data['Subject'].str.contains(\"Referral Status\").sum())\n",
    "print(data['Subject'].str.contains(\"Status of your referral\").sum())\n",
    "print(data['Subject'].str.contains(\"referral status\").sum())\n",
    "print(data['Subject'].str.contains(\"Status of your Referral\").sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data[data['Subject'].str.contains(\"Referral Status\", na = False)].index, inplace = True)\n",
    "data.drop(data[data['Subject'].str.contains(\"Status of your referral\", na = False)].index, inplace = True)\n",
    "data.drop(data[data['Subject'].str.contains(\"referral status\", na = False)].index, inplace = True)\n",
    "data.drop(data[data['Subject'].str.contains(\"Status of your Referral\", na = False)].index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Looking for most frequent Emails subjects. If a particular subject is appearing too many times, probably that consists of long chain of emails and fo no good to our cause. Hence remove the emails with that subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['Subject'].value_counts(sort = True)[:20]\n",
    "#data['Subject'].value_counts(sort = True)[:20].sum()\n",
    "subs = pd.Series(data['Subject'].value_counts(sort = True)[:20])\n",
    "subs = subs.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing a method to remove most frequent subjects from the database\n",
    "def remove_sub(sub):\n",
    "    data.drop(data[data['Subject'].str.contains(sub, na = False)].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing a few subjects, as we may lose more information \n",
    "rem_list = ['RE: Referral Bonus','RE: Referral','RE: Referral Question','Referral','Referral Question']\n",
    "for sub in rem_list:\n",
    "    subs.remove(str(sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akadali\\Anaconda\\lib\\site-packages\\pandas\\core\\strings.py:1843: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  return func(self, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "for sub in subs:\n",
    "    remove_sub(str(sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now if we check the size of a data frame...991 records should have been removed.\n",
    "#data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remove Emails sent from US Talent Referral Program and 'Replied Emails'. As these are the response sent by the Talent Referral team and we do not need them for now, as we are trying to work on questions here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['From: (Name)'].str.contains('US Talent Referral Program').sum()\n",
    "data.drop(data[data['From: (Name)'].str.contains('US Talent Referral Program', na = False)].index,inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dropping emails with Subjects start with 'RE' and 'Re' as we need only FCQs (First Contact Queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['Subject'].str.startswith(\"Re:\").sum()\n",
    "#data['Subject'].str.startswith(\"RE:\").sum()\n",
    "data.drop(data[data['Subject'].str.startswith(\"RE:\", na = False)].index, inplace = True)\n",
    "data.drop(data[data['Subject'].str.startswith(\"Re:\", na = False)].index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check if there are any emails with subject 'Action Required'. These are the validation emails sent out to recruiters by the team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.drop(data[data['Subject'].str.contains(\"Action Required\", na = False)].index, inplace = True)\n",
    "data.drop(data[data['Subject'].str.contains(\"ACTION REQUIRED\", na = False)].index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Look at the Most frequent statuses again adn remove if there are any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['Subject'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data[data['Subject'].str.startswith(\"FW: [EXT] Thank you for your referral to Deloitte\", na = False)].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "US Talent Referral Program                                                948\n",
       "US Talent Referral Program (US - Stamford)                                863\n",
       "PPSS Field II                                                              35\n",
       "Agarwal, Jaideep                                                           13\n",
       "Mercer, Tiffany;Callahan, Jane                                             12\n",
       "                                                                         ... \n",
       "Loreto, Evian (US - Denver);Breck, Kim (US - Minneapolis)                   1\n",
       "US Talent Referral Program (US - Stamford);Loreto, Evian (US - Denver)      1\n",
       "Garren, Kacee (US - Arlington)                                              1\n",
       "Singhal, Prashant                                                           1\n",
       "Johansen, Alex                                                              1\n",
       "Name: To: (Name), Length: 188, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['To: (Name)'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Removing all rows/records that are not sent to US Talent Referral Program mailbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing all rows/records that are not sent to US Talent Referral Program mailbox\n",
    "data.drop(data[~data['To: (Name)'].str.contains(\"US Talent Referral Program\", na = False)].index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Removing emails received from \"Callahan, Jane\", \"Tiffany, Mercer\", \"US CTS Talent R&A\", 'Deloitte (O365D) on Yammer'm \"YourOnlineProfessor\", \"US Talent CIC Inbox\", \"US R10 Referral Response\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_list2 = [\"Callahan, Jane\", \"Tiffany, Mercer\", \"US CTS Talent R&A\", \n",
    "             \"Deloitte (O365D) on Yammer \", \"YourOnlineProfessor\", \n",
    "             \"US Talent CIC Inbox\", \"US R10 Referral Response\", \"Deloitte Global Recruiting\", \"Yammer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_from(fname):\n",
    "    data.drop(data[data['From: (Name)'].str.contains(fname, na = False)].index, inplace = True)\n",
    "\n",
    "for fname in fname_list2:\n",
    "    remove_from(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remove unecessary variables from the remaining columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['From: (Name)', 'From: (Address)', 'From: (Type)', 'To: (Name)', 'To: (Address)', 'To: (Type)'], \n",
    "          axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Signatures from the emails\n",
    "\n",
    "Signatures for all emails should be removed as we are going to need only the emails body texts for our training data. Removing/separating signatures from the actual email bodies involves below steps\n",
    "\n",
    "* Breakdown email body into paragraphs\n",
    "\n",
    "* Calculate POS of all words using nltk\n",
    "\n",
    "* Any paragraph that doesn't have enough/no verbs is a signature and that should be trimmed off\n",
    "\n",
    "* Verb count for each paragraph/sentence is calculated, the second para/line where we have no verbs, that's where our email body ends and signature starts\n",
    "\n",
    "* In summary, it discards blocks that do not contain enough verbs to be considered a message block, being treated as signature blocks instead.\n",
    "\n",
    "* Information source: https://kinoshita.eti.br/2017/06/14/how-to-remove-the-signature-from-emails-with-nlp.html\n",
    "\n",
    "* Below function exactly does that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import blankline_tokenize\n",
    "from collections import Counter\n",
    "def trim_signature(email):\n",
    "    paras = blankline_tokenize(str(email)) #divdes the email into small paragraphs\n",
    "    v_count = []                           #List of verbs count for each line\n",
    "    lines = []                             #List of lines\n",
    "    line_count = 0                         #Number of Lines\n",
    "    lv_count = 0                           #Lines with verbs\n",
    "    nv_count = 0                           #Lines with no-verbs\n",
    "    for line in paras:\n",
    "        line_count += 1\n",
    "        tokens = nltk.word_tokenize(line.lower())  \n",
    "        text = nltk.Text(tokens)\n",
    "        tags = nltk.pos_tag(text)\n",
    "        counts = Counter(tag for word,tag in tags)\n",
    "        verbs = counts['VB']+counts['VBD']+counts['VBG']+counts['VBN']+counts['VBP']+counts['VBZ']\n",
    "        if verbs != 0 :\n",
    "            lv_count += 1\n",
    "            lines.append(line)\n",
    "            v_count.append(verbs)\n",
    "        else:\n",
    "            nv_count += 1\n",
    "        if nv_count == 2:\n",
    "            break\n",
    "    return \" \".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import blankline_tokenize\n",
    "from collections import Counter\n",
    "def trim_signatures2(email):\n",
    "    paras = blankline_tokenize(str(email)) #divdes the email into small paragraphs\n",
    "    v_count = []                           #List of verbs count for each line\n",
    "    lines = []                             #List of lines\n",
    "    line_count = 0                         #Number of Lines\n",
    "    lv_count = 0                           #Lines with verbs\n",
    "    nv_count = 0                           #Lines with no-verbs\n",
    "    for line in paras:\n",
    "        line_count += 1\n",
    "        tokens = nltk.word_tokenize(line.lower())  \n",
    "        text = nltk.Text(tokens)\n",
    "        tags = nltk.pos_tag(text)\n",
    "        counts = Counter(tag for word,tag in tags)\n",
    "        verbs = counts['VB']+counts['VBD']+counts['VBG']+counts['VBN']+counts['VBP']+counts['VBZ']\n",
    "        print(\"Sentence#\",line_count)\n",
    "        print(\"No. of Verbs\", verbs)\n",
    "    return paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence# 1\n",
      "No. of Verbs 0\n",
      "Sentence# 2\n",
      "No. of Verbs 2\n",
      "Sentence# 3\n",
      "No. of Verbs 0\n",
      "Sentence# 4\n",
      "No. of Verbs 2\n",
      "Sentence# 5\n",
      "No. of Verbs 0\n",
      "Sentence# 6\n",
      "No. of Verbs 5\n",
      "Sentence# 7\n",
      "No. of Verbs 1\n",
      "Sentence# 8\n",
      "No. of Verbs 0\n",
      "Sentence# 9\n",
      "No. of Verbs 2\n",
      "Sentence# 10\n",
      "No. of Verbs 0\n",
      "Sentence# 11\n",
      "No. of Verbs 1\n",
      "Sentence# 12\n",
      "No. of Verbs 0\n",
      "Sentence# 13\n",
      "No. of Verbs 1\n",
      "Sentence# 14\n",
      "No. of Verbs 0\n",
      "Sentence# 15\n",
      "No. of Verbs 4\n",
      "Sentence# 16\n",
      "No. of Verbs 1\n",
      "Sentence# 17\n",
      "No. of Verbs 0\n",
      "Sentence# 18\n",
      "No. of Verbs 0\n",
      "Sentence# 19\n",
      "No. of Verbs 1\n",
      "Sentence# 20\n",
      "No. of Verbs 1\n"
     ]
    }
   ],
   "source": [
    "k = trim_signatures2(data['Body'].iloc[123])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi there,',\n",
       " 'Can you let me know the referral bonus for an international tax manager for the OC / LA office?',\n",
       " 'Thank you',\n",
       " 'From: Chavira-Aubel, Sarah (US - San Diego) \\r\\nSent: Friday, June 7, 2019 3:27 PM\\r\\nTo: Chaudhry, Ausaff Arshad (US - San Jose) <achaudhry@deloitte.com>\\r\\nSubject: RE: Referral bonus',\n",
       " 'Hi Ausaff,',\n",
       " 'Please email USTalentReferralProgram@deloitte.com <mailto:USTalentReferralProgram@deloitte.com>  and they can let you know when it will be paid out.',\n",
       " 'Thank you and have a great weekend!',\n",
       " 'Sarah Chavira-Aubel | Deloitte\\x95',\n",
       " 'Lead Experienced Recruiting Specialist | Tax',\n",
       " 'Direct: +1 619-237-6728 | Mobile: +1 858-281-9314',\n",
       " 'schaviraaubel@deloitte.com <mailto:schaviraaubel@deloitte.com>  | www.deloitte.com <http://www.deloitte.com/>',\n",
       " 'Explore Deloitte Tax LLP <https://usrecruiting.deloitte.com/experienced-hire-tax-talent-information>',\n",
       " 'From: Chaudhry, Ausaff Arshad (US - San Jose) \\r\\nSent: Thursday, June 6, 2019 3:30 PM\\r\\nTo: Chavira-Aubel, Sarah (US - San Diego) <schaviraaubel@deloitte.com <mailto:schaviraaubel@deloitte.com> >\\r\\nSubject: Referral bonus',\n",
       " 'Hi Sarah,',\n",
       " 'Can you let me know what the referral bonus will likely be for Ning?',\n",
       " 'Hope all is well.',\n",
       " 'Thanks!',\n",
       " 'Ausaff Chaudhry | Deloitte. \\r\\nInternational Tax Services',\n",
       " 'Tel: +1 (408) 704-2491\\r\\nFax: +1 (408) 704-8235 | Email: achaudhry@deloitte.com <mailto:achaudhry@deloitte.com>  \\r\\nWebsite: www.deloitte.com <http://www.deloitte.com/>',\n",
       " '225 West Santa Clara Street | San Jose, CA 95113-1728']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the above method\n",
    "data['email_cleaned'] = data['Body'].apply(lambda x: trim_signature(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Email body texts have strange characters like X97,\\X94,\\X93. They should be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def text_clean1(text):\n",
    "    text = re.sub(\"[\\x97]\",\"'\", text)\n",
    "    text = re.sub(\"[\\x96]\",\"'\", text)\n",
    "    text = re.sub(\"[\\x95]\",\"'\", text)\n",
    "    text = re.sub(\"[\\x94]\",\"'\", text)\n",
    "    text = re.sub(\"[\\x93]\",\"'\", text)\n",
    "    text = re.sub(\"[\\x92]\",\"'\", text)\n",
    "    text = re.sub(\"[\\x91]\",\"'\", text)\n",
    "    return text\n",
    "\n",
    "#apply the function\n",
    "data['email_cleaned'] = data['email_cleaned'].apply(lambda x: text_clean1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As part of the BTA Core School Referral for Spring 2019, I referred Niki Zhao <https://people.deloitteresources.com/dpn/index.html#/Profile/nikzhao>  (zhao13@iu.edu <mailto:zhao13@iu.edu> ) from Indiana University in Fall 2018. She was offered the position and joined the firm in Feb 2019. I haven't received any communication from the Referral program ever since and wanted to follow up with you regarding this. I reached out to Delaine McAndrews <mailto:demcandrews@DELOITTE.com>  (IU School Recruiter) in order to refer Niki and was told that she was selected to interview for round 1. Do you happen to have any records of the same?\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sample test\n",
    "data['email_cleaned'].iloc[87]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calculating the number of words and characters for every email...after they are cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a column of with word counts and char counts in each sentence\n",
    "data['tw_pre_cleaning'] = data['Body'].str.split().str.len()\n",
    "data['tc_pre_cleaning'] = data['Body'].str.strip().str.len()\n",
    "data['tw_post_cleaning'] = data['email_cleaned'].str.split().str.len()\n",
    "data['tc_post_cleaning'] = data['email_cleaned'].str.strip().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the unnecesary columns\n",
    "#data.drop(['totalwords', 'totalchars'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average no. of words in an email before cleaning the data 197.87\n",
      "Average no. of words in an email after cleaning the data 52.57\n",
      "Difference or unnecessary words removed 145.3\n"
     ]
    }
   ],
   "source": [
    "print(\"Average no. of words in an email before cleaning the data\", np.round(data['tw_pre_cleaning'].mean(),2))\n",
    "print(\"Average no. of words in an email after cleaning the data\", np.round(data['tw_post_cleaning'].mean(), 2))\n",
    "print(\"Difference or unnecessary words removed\", np.round(data['tw_pre_cleaning'].mean()-data['tw_post_cleaning'].mean(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defeinitions to calculate Verbs and lines in all email inquiries\n",
    "def ver_counter(email):\n",
    "    paras_ = blankline_tokenize(str(email)) #divdes the email into small paragraphs\n",
    "    verbs_ = 0\n",
    "    for line in paras_:\n",
    "        tokens = nltk.word_tokenize(line.lower())  \n",
    "        text = nltk.Text(tokens)\n",
    "        tags = nltk.pos_tag(text)\n",
    "        counts = Counter(tag for word,tag in tags)\n",
    "        verbs_ = counts['VB']+counts['VBD']+counts['VBG']+counts['VBN']+counts['VBP']+counts['VBZ']\n",
    "    return verbs_\n",
    "\n",
    "\n",
    "def line_counter(email):\n",
    "    paras_ = blankline_tokenize(str(email)) #divdes the email into small paragraphs\n",
    "    return len(paras_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['verbs'] = data['email_cleaned'].apply(lambda x:ver_counter(x))\n",
    "data['lines'] = data['email_cleaned'].apply(lambda x:line_counter(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.577136514983351"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['verbs'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resetting the index and dropping the column\n",
    "data = data.reset_index()\n",
    "data.drop(['index'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Backup #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_backup1 = data\n",
    "data_backup1.to_csv('C:\\\\Users\\\\akadali\\\\Desktop\\\\Deep_NLP\\\\MLG_Capstone_ChatBot\\\\ChatBot_GoogleW2V\\\\data_Preprocessed.csv', \n",
    "                    index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data[\"tw_post_cleaning\"], bins=30)\n",
    "plt.title(\"Word count Distribution across emails\", fontsize = 15)\n",
    "plt.axvline(210)\n",
    "plt.xticks(np.arange(0,500,30))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "sns.set(rc={'figure.figsize':(20,6)})\n",
    "sns.boxplot(x=data['tw_post_cleaning'], color = 'm')\n",
    "#plt.axvline(36,0, linestyle = \"--\", color = 'red')\n",
    "plt.title(\"Word count Distribution across emails\", fontsize = 15)\n",
    "plt.xticks(np.arange(0,450,30))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From the above it is evident that emails with more than 120 words are outliers\n",
    "* These emails to be removed as we are going to need only those emails with minimal text that can be given as input questions to the bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the records with more than 120 words - There are 80 such rferral queries with more than 80 words\n",
    "data.drop(data[data['tw_post_cleaning']>120].index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Count Distributions post removing the emails with large texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data[\"tw_post_cleaning\"], bins=30)\n",
    "plt.title(\"Word count Distribution across emails\", fontsize = 15)\n",
    "#plt.xticks(np.arange(0,500,30))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data[\"tc_post_cleaning\"], bins = 200)\n",
    "plt.title(\"Question Length\", fontsize = 20)\n",
    "#plt.xticks(np.arange(0,500,30))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "sns.set(rc={'figure.figsize':(20,6)})\n",
    "sns.boxplot(x=data['tw_post_cleaning'], color = 'm')\n",
    "#plt.axvline(36,0, linestyle = \"--\", color = 'red')\n",
    "plt.title(\"Word count Distribution across emails\", fontsize = 15)\n",
    "plt.xticks(np.arange(0,120,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Majority of emails have text lengths between 30 and 60 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data[\"verbs\"], bins=30)\n",
    "plt.title(\"Verbs Distribution across emails\", fontsize = 15)\n",
    "#plt.xticks(np.arange(0,500,30))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Backup #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a back-up\n",
    "#data_step1 = data\n",
    "#Exporting to a dataframe\n",
    "data_step1.to_csv('C:\\\\Users\\\\akadali\\\\Desktop\\\\Deep_NLP\\\\MLG_Capstone_ChatBot\\\\ChatBot_GoogleW2V\\\\data_Preprocessed2.csv', index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_step1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Mining/Text Analytics\n",
    "-------\n",
    "\n",
    "Data Clean-up\n",
    "**Common data cleaning steps on all text:**\n",
    "* Make text all lower case\n",
    "* Remove punctuation\n",
    "* Remove numerical values\n",
    "* Remove common non-sensical text (/n)\n",
    "* Remove contractions\n",
    "* Remove stop words\n",
    "* Tokenize text\n",
    "\n",
    "**More data cleaning steps after tokenization:**\n",
    "* Stemming / lemmatization\n",
    "* Parts of speech tagging\n",
    "* Create bi-grams or tri-grams\n",
    "* Deal with typos\n",
    "* And more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting data to lower case\n",
    "col = 'email_cleaned'\n",
    "data[col] = data[col].apply(lambda x: x.lower())\n",
    "\n",
    "#Removing Punctuations\n",
    "puncs = string.punctuation\n",
    "puncs = puncs + \"’'`\"\n",
    "data[col] = data[col].apply(lambda x: re.sub('[%s]'%re.escape(puncs), ' ', x))\n",
    "\n",
    "#Removing Stop Words\n",
    "stops = nltk.corpus.stopwords.words('english')\n",
    "data[col] = data[col].apply(lambda x: \" \".join(w for w in x.split() if w not in stops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The text data has a lot of contractions and all of them need to be cleared to \n",
    "#avoid any unsual characters after preprocessing the text\n",
    "def text_clean(text):\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"won\\'t\", \"will not\", text)\n",
    "    text = re.sub(r\"\\n\", \" \", text)\n",
    "    text = re.sub(r\"\\r\", \" \", text)\n",
    "    return text\n",
    "\n",
    "#Applying the above function\n",
    "data[col] = data[col].apply(lambda x: text_clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We still have some extra spaces present in the data. Let’s remove them\n",
    "#Removing extra spaces that might have been created while removing punctuations\n",
    "data[col] = data[col].apply(lambda x: re.sub('  +',' ',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing the text\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "corpus = \" \".join(data[col])\n",
    "tokens = word_tokenize(corpus)\n",
    "print(\"Total number of words in the corpus - \", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get the unique list of words\n",
    "def unique_list(text):\n",
    "    unique_list = []\n",
    "    for w in text.split():\n",
    "        if w not in unique_list:\n",
    "            unique_list.append(w)\n",
    "    return unique_list\n",
    "\n",
    "#Getting Lemmas for corpus tokens\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "lems = \" \".join(lemma.lemmatize(w, pos = 'v') for w in tokens)\n",
    "unique_lems = unique_list(lems)\n",
    "print(\"Number of unique (root) words in corpus - \", len(unique_lems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding word frequencies of all words in the corpus tokens\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist()\n",
    "\n",
    "for word in tokens:\n",
    "    fdist[word] += 1\n",
    "    \n",
    "\n",
    "#Top 20 tokens with highest frequency \n",
    "fdist_top20 = fdist.most_common(20)\n",
    "word_freq = pd.DataFrame(fdist_top20)\n",
    "word_freq.columns = ['word','freq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar Chart to show the top 20 most frequent words\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "plt.barh(word_freq['word'],word_freq['freq'])\n",
    "plt.xlabel(\"Top_Words\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Top Words Frequency\", fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's Generate a wordcloud to observer the highligted words over the entire corpus\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white', \n",
    "                stopwords = stops, \n",
    "                min_font_size = 10).generate(corpus)\n",
    "\n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.title(\"Unigram Wordcloud\")\n",
    "plt.tight_layout(pad = 0) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets get the bigrams and trigrams\n",
    "from nltk import ngrams\n",
    "bigrams = list(ngrams(corpus.split(), 2))\n",
    "trigrams = list(ngrams(corpus.split(), 3))\n",
    "\n",
    "bigrams = pd.DataFrame(bigrams)\n",
    "bigrams = bigrams[0] + \"_\" + bigrams[1] \n",
    "\n",
    "bigram_words = ' '\n",
    "for val in bigrams:\n",
    "    val = str(val)\n",
    "    tokens = val.split()\n",
    "    bigram_words += \" \".join(tokens)+\" \"\n",
    "    \n",
    "#Top 20 bigrams\n",
    "bigrams.value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white', \n",
    "                stopwords = stops, \n",
    "                min_font_size = 10).generate(bigram_words) \n",
    "\n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.title(\"Bigram Word Cloud\", fontsize = 15)\n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trigrams word clouds\n",
    "trigrams = pd.DataFrame(trigrams)\n",
    "trigrams = trigrams[0]+'_'+trigrams[1]+'_'+trigrams[2]\n",
    "trigrams.value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_words = ' '\n",
    "for val in trigrams:\n",
    "    val = str(val)\n",
    "    tokens = val.split()\n",
    "    trigram_words += \" \".join(tokens)+\" \"\n",
    "    \n",
    "bigram_words\n",
    "wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white', \n",
    "                stopwords = stops, \n",
    "                min_font_size = 10).generate(trigram_words) \n",
    "\n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.title(\"Trigrams Wordcloud\", fontsize = 15)\n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try LDA model to determine the topics in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's attempt to use CountVectorizer document-term matrix and gensim models for topic modelling\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(stop_words = 'english')\n",
    "data_cv = cv.fit_transform(data[col])\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns = cv.get_feature_names())\n",
    "#data_dtm.index = qa_data.index\n",
    "#data_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "# One of the required inputs is a term-document matrix\n",
    "tdm = data_dtm.transpose()\n",
    "tdm.head()\n",
    "\n",
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)\n",
    "\n",
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "#cv.vocabulary_\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term)\n",
    "\n",
    "#LDA for num_topics = 2\n",
    "lda_2 = models.LdaModel(corpus = corpus, id2word = id2word, num_topics = 2, passes = 10)\n",
    "lda_2.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA for num_topics = 3\n",
    "lda = models.LdaModel(corpus = corpus, id2word = id2word, num_topics = 3, passes = 10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA for num_topics = 4\n",
    "lda = models.LdaModel(corpus = corpus, id2word = id2word, num_topics = 4, passes = 10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)\n",
    "\n",
    "data_nouns_adj = pd.Series(data[col].apply(nouns_adj))\n",
    "#data_nouns_adj.shape\n",
    "data_nouns_adj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"data_nouns_adj\"\"\"\n",
    "\"\"\"LDA with Nouns and Adjectives\"\"\"\n",
    "cvna = CountVectorizer(stop_words = 'english')\n",
    "dat_cvna = cvna.fit_transform(data_nouns_adj)\n",
    "data_dtna = pd.DataFrame(dat_cvna.toarray(), columns = cvna.get_feature_names())\n",
    "\n",
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"LDA with nouns & adjectives - no. of topics = 2\"\"\"\n",
    "ldna = models.LdaModel(corpus = corpusna, id2word = id2wordna, num_topics = 2, passes = 10)\n",
    "ldna.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"LDA with nouns & adjectives - no. of topics = 3\"\"\"\n",
    "ldna_3 = models.LdaModel(corpus = corpusna, id2word = id2wordna, num_topics = 3, passes = 10)\n",
    "ldna_3.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Topic#1 - Job positions, resume submissions\n",
    "* Topic#2 - Referral bonus\n",
    "* Topic#3 - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"LDA with nouns & adjectives - no. of topics = 4\"\"\"\n",
    "ldna_4 = models.LdaModel(corpus = corpusna, id2word = id2wordna, num_topics = 4, passes = 10)\n",
    "ldna_4.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"LDA with nouns & adjectives - no. of topics = 8\"\"\"\n",
    "ldna_8 = models.LdaModel(corpus = corpusna, id2word = id2wordna, num_topics = 8, passes = 10)\n",
    "ldna_8.print_topics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
