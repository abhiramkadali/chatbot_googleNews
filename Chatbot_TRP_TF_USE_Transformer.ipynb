{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from absl import logging\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "The Universal Sentence Encoder encodes text into high-dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks.\n",
    "\n",
    "The model is trained and optimized for greater-than-word length text, such as sentences, phrases or short paragraphs. It is trained on a variety of data sources and a variety of tasks with the aim of dynamically accommodating a wide variety of natural language understanding tasks. The input is variable length English text and the output is a 512 dimensional vector. We apply this model to the STS benchmark for semantic similarity, and the results can be seen in the example notebook made available. The universal-sentence-encoder model is trained with a deep averaging network (DAN) encoder.\n",
    "\n",
    "To learn more about text embeddings, refer to the TensorFlow Embeddings documentation. Our encoder differs from word level embedding models in that we train on a number of natural language prediction tasks that require modeling the meaning of word sequences rather than just individual words. Details are available in the paper \"Universal Sentence Encoder\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module https://tfhub.dev/google/universal-sentence-encoder-large/5 loaded\n"
     ]
    }
   ],
   "source": [
    "#Loading model from USE DAN (Deep Averaging Network)\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/5\" \n",
    "model = hub.load(module_url)\n",
    "print (\"module %s loaded\" % module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to embed inputs\n",
    "def embed(input):\n",
    "    return model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/akadali/Desktop/Deep_NLP/MLG_Capstone_ChatBot/ChatBot_GoogleW2V')\n",
    "data = pd.read_csv('chatbot_train.csv', encoding = 'latin1')\n",
    "data = data[['question', 'answer', 'intent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~’'`\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "stops = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import string\n",
    "puncs = string.punctuation\n",
    "#Adding additional punctuations\n",
    "puncs = puncs + \"’'`\" \n",
    "print(puncs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "-----\n",
    "\n",
    "* This dataset has a lot of contractions, hence all of them need to be removed/replaced within the dataset. The below function replaces the contractions with corresponding word pairs.\n",
    "\n",
    "* Also, there are quite a few abbreviations such as U.S., US, USI to be substituted with their expanded forms\n",
    "\n",
    "* removing additional spaces, if there are any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def text_clean(text):\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub('[%s]'%re.escape(puncs), ' ', text)\n",
    "    #convert u.s or us  to 'United States'\n",
    "    text = re.sub(r\" U.S. \", \" united states \", text)\n",
    "    text = re.sub(r\" US \", \" united states \", text)\n",
    "    text = re.sub(r\" USI \", \" united states india \", text)\n",
    "    text = re.sub(r\" +\",\" \", text)\n",
    "    #Removing additional characters that captured during import\n",
    "    text = re.sub(\"[\\x97]\",\"'\", text)\n",
    "    text = re.sub(\"[\\x96]\",\"'\", text)\n",
    "    text = re.sub(\"[\\x95]\",\"'\", text)\n",
    "    text = re.sub(\"[\\x94]\",\"'\", text)\n",
    "    text = re.sub(\"[\\x93]\",\"'\", text)\n",
    "    text = re.sub(\"[\\x92]\",\"'\", text)\n",
    "    text = re.sub(\"[\\x91]\",\"'\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Cleaning(2)\n",
    "----------------------------\n",
    "* Tokenizing the words\n",
    "* Lemmatization\n",
    "* removing numeric digits and punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* import test data\n",
    "* create test question embeddings\n",
    "* create a new column for new answers\n",
    "* get the answers in that column\n",
    "* compare both answers\n",
    "* get the results\n",
    "* Get the cosine values as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text):\n",
    "    #tokenizing\n",
    "    tokens = word_tokenize(text)\n",
    "    #converting to lower case and lemmatization\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    filtered_tokens = []\n",
    "    #Remove anything but alphabets - numbers, punctuations etc\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    #return list of lists for embedding vectors\n",
    "    return \" \".join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the questions dataset\n",
    "data['question'] = data['question'].apply(lambda x: text_clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess the questions\n",
    "data['question'] = data['question'].apply(lambda x:text_preprocess(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Embeddings using pretrained google universal-sentence-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to embed inputs\n",
    "def embed(input):\n",
    "    return model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.seterr(divide='ignore', invalid='ignore')\n",
    "question_vectors = [embed([x]) for x in data['question']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'can you confirm when jennie payout will be'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['question'][144]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 512), dtype=float32, numpy=\n",
       "array([[-0.05937516, -0.07139486, -0.03580907, -0.02181975,  0.04792665,\n",
       "        -0.0549308 , -0.00814513, -0.00288107,  0.01621236,  0.06205643,\n",
       "         0.00832495,  0.01106013,  0.03006287, -0.03137842, -0.00210727,\n",
       "         0.03081903, -0.0387282 ,  0.03473219,  0.01469928,  0.0431647 ,\n",
       "        -0.00048844,  0.06611931, -0.0083472 ,  0.01772776, -0.08686357,\n",
       "         0.00481038, -0.01594081,  0.03066936,  0.01401082,  0.00223883,\n",
       "         0.04004965,  0.00769123, -0.03169547,  0.04066599, -0.01992467,\n",
       "         0.04570906, -0.00786444,  0.08135858, -0.02986662, -0.00229437,\n",
       "        -0.02533258,  0.00544696,  0.07811685,  0.01056373, -0.04172884,\n",
       "        -0.00344714,  0.03654783,  0.04101761,  0.01500739, -0.03979726,\n",
       "        -0.05531175, -0.04886104,  0.05610216, -0.00932195, -0.05707739,\n",
       "        -0.05330759,  0.01215209, -0.06623979, -0.047648  , -0.04336387,\n",
       "         0.10992612,  0.01377444, -0.01867487, -0.00571322,  0.05534788,\n",
       "         0.01357876, -0.01893349,  0.00466477, -0.01862493,  0.03198985,\n",
       "         0.05067926,  0.0174484 ,  0.06112334, -0.07374628, -0.027833  ,\n",
       "         0.07211826, -0.01440033, -0.02751158,  0.02329024, -0.02241914,\n",
       "         0.03930983, -0.04708865,  0.04494821, -0.07898122,  0.00656467,\n",
       "         0.06875859,  0.06519233,  0.07218235, -0.03017382, -0.0757008 ,\n",
       "         0.02137143, -0.03702784,  0.0351101 , -0.03481097, -0.04201256,\n",
       "         0.00424597,  0.01191392,  0.01799265,  0.08463101,  0.04194496,\n",
       "        -0.0044722 ,  0.03830542, -0.00210856, -0.03318434,  0.07726995,\n",
       "        -0.06617959,  0.02883137, -0.01307134,  0.00282539,  0.04281801,\n",
       "        -0.05822898, -0.07323091, -0.05117626, -0.03936206,  0.02916691,\n",
       "        -0.07111462,  0.01838239,  0.04122371,  0.03116319, -0.05064484,\n",
       "         0.02827019, -0.00305802,  0.00602141,  0.00335494, -0.00208902,\n",
       "        -0.02162334,  0.03123497,  0.05367119, -0.00527617,  0.01999136,\n",
       "         0.01933371, -0.06916815, -0.0210385 , -0.0580488 ,  0.08400325,\n",
       "         0.03628819,  0.10075991, -0.06714513, -0.00670535,  0.06445815,\n",
       "        -0.01673121, -0.00614294, -0.00809745, -0.03333345,  0.00124237,\n",
       "         0.01875848,  0.05553186, -0.07524096, -0.02577071, -0.00904023,\n",
       "         0.01233926,  0.06641483,  0.04364633, -0.05106845, -0.07173836,\n",
       "         0.06300834,  0.05876194, -0.03903807, -0.00989995,  0.00232816,\n",
       "        -0.02070099,  0.02775154,  0.0197427 ,  0.05965697,  0.00541744,\n",
       "         0.10692675, -0.02964015, -0.0245686 , -0.00156008, -0.02010264,\n",
       "        -0.03570102,  0.03722754, -0.03064957,  0.0100523 , -0.04673404,\n",
       "        -0.04427961, -0.0039881 ,  0.01765298,  0.01388317, -0.04362755,\n",
       "        -0.03016842, -0.03618477, -0.062852  ,  0.01271988, -0.02533327,\n",
       "         0.01577961,  0.00970163,  0.02516817,  0.03876943,  0.03030027,\n",
       "         0.07527774, -0.01027368,  0.03668702, -0.04284415,  0.0366593 ,\n",
       "        -0.05266829, -0.01212507,  0.0991241 , -0.0101943 ,  0.00468056,\n",
       "        -0.01912654, -0.03261817,  0.04120183, -0.01763684, -0.04609821,\n",
       "        -0.09088486, -0.03401389,  0.0530203 ,  0.06621658, -0.02145744,\n",
       "         0.07964174,  0.01648532, -0.00268417,  0.02019728, -0.02113346,\n",
       "         0.03814067, -0.00611041,  0.00844102, -0.0231056 ,  0.05978894,\n",
       "         0.05884942, -0.00250805,  0.09907166,  0.00793333, -0.00020854,\n",
       "        -0.00587343, -0.01459266, -0.07665227, -0.0173834 ,  0.0116655 ,\n",
       "        -0.01811885,  0.01114269, -0.054438  ,  0.04635911,  0.01746457,\n",
       "        -0.01021097, -0.01564729,  0.00800667, -0.08852984, -0.02562176,\n",
       "        -0.07661258,  0.03208309,  0.05893401, -0.04277106, -0.02495624,\n",
       "         0.05892798,  0.03826461, -0.08607981,  0.01074155, -0.10010005,\n",
       "         0.00056379, -0.01207063,  0.02342783,  0.07221207,  0.01861136,\n",
       "         0.04702894,  0.0428187 ,  0.0549505 , -0.03794493, -0.02564088,\n",
       "         0.00787218,  0.0079811 ,  0.00711932, -0.0408798 ,  0.01441546,\n",
       "         0.01545296,  0.00393882, -0.10150795,  0.00581288,  0.02370788,\n",
       "         0.08655837, -0.04415755,  0.0237863 , -0.02816651,  0.03657797,\n",
       "        -0.03176953,  0.05154522, -0.07397728, -0.01197084, -0.04741568,\n",
       "         0.0529149 ,  0.02564613,  0.04657809, -0.03442569,  0.04858512,\n",
       "        -0.04301008,  0.09214823, -0.03604368,  0.00536404,  0.01994008,\n",
       "        -0.04377731, -0.06301813, -0.0520237 , -0.0303247 ,  0.02276479,\n",
       "        -0.00794538, -0.01585723,  0.050862  , -0.00611465, -0.02339764,\n",
       "        -0.01332679,  0.00383795, -0.02776898,  0.00241206,  0.0231989 ,\n",
       "        -0.02967892,  0.03052877,  0.09029406, -0.02557969, -0.00863173,\n",
       "        -0.00778408,  0.04666644,  0.04106952, -0.03775879,  0.02668466,\n",
       "         0.02417973,  0.02441212,  0.00425073, -0.02212202, -0.08500557,\n",
       "        -0.01326376, -0.016961  , -0.02836302,  0.11037667,  0.04028071,\n",
       "         0.01266555, -0.12641127, -0.07654402, -0.00116701,  0.07860588,\n",
       "        -0.02265985, -0.02910902, -0.00435016,  0.05025868,  0.01459281,\n",
       "        -0.03871525, -0.0058744 , -0.08507829, -0.00405515, -0.03010738,\n",
       "         0.02034197,  0.03957183,  0.07662425, -0.01424367, -0.12919594,\n",
       "         0.02469032,  0.0189091 ,  0.06021791,  0.01534985,  0.06471565,\n",
       "         0.03509331,  0.01940025,  0.02104686, -0.029224  ,  0.06623456,\n",
       "         0.03324864,  0.07951732, -0.00469734,  0.00771914,  0.03290038,\n",
       "        -0.02996374,  0.00856503,  0.00856688,  0.00203304, -0.06272154,\n",
       "         0.01641129,  0.02940957,  0.07116059, -0.07374115, -0.05560743,\n",
       "        -0.04654843, -0.02931953,  0.04764857, -0.00663729,  0.07489732,\n",
       "         0.03166665, -0.00455897, -0.00289854, -0.00819712, -0.07396424,\n",
       "         0.05347809, -0.06253697,  0.02844827, -0.02156514, -0.00982796,\n",
       "        -0.01473454,  0.02841508, -0.04917265,  0.02746773, -0.03801573,\n",
       "        -0.06813471,  0.01888374,  0.02260965,  0.09946054, -0.00784843,\n",
       "         0.0041292 ,  0.00386386,  0.02124523, -0.08088908,  0.03178715,\n",
       "        -0.03575424,  0.01423377,  0.06812503,  0.0162183 ,  0.05958556,\n",
       "         0.01913526, -0.03707412,  0.05696508,  0.01872499, -0.0457219 ,\n",
       "        -0.00871625,  0.04306114, -0.03275803,  0.03013212,  0.05460342,\n",
       "         0.06820465, -0.03221378, -0.04016409,  0.02927399,  0.03701357,\n",
       "         0.03835681,  0.00413324, -0.02153489,  0.0456145 ,  0.02040518,\n",
       "         0.04611553,  0.01004279, -0.05974487,  0.06981122,  0.07900949,\n",
       "        -0.07734245,  0.03435516,  0.11030352, -0.0170724 , -0.00031916,\n",
       "        -0.03338189, -0.05332255, -0.06638948,  0.03027538, -0.00694332,\n",
       "        -0.05581454,  0.09327952, -0.08810344, -0.04576055, -0.0167025 ,\n",
       "        -0.01917391,  0.07056441, -0.01922095,  0.00692238,  0.03504502,\n",
       "         0.03786322,  0.00897341,  0.00131537,  0.00706513, -0.03821413,\n",
       "         0.03116397,  0.04910108,  0.0545572 ,  0.01276355,  0.02626368,\n",
       "         0.01539417, -0.01605774,  0.01612887,  0.0423424 , -0.02941693,\n",
       "        -0.03080727,  0.05409398,  0.03115165,  0.03792322,  0.08798064,\n",
       "        -0.07159316,  0.00253871, -0.01701954, -0.01438861, -0.07267343,\n",
       "        -0.03055481, -0.0283656 , -0.10572026, -0.07843766,  0.02019129,\n",
       "         0.059512  , -0.0717565 , -0.00866885, -0.00469354, -0.0954423 ,\n",
       "         0.00468715,  0.09253882,  0.02074686, -0.0548701 , -0.0234106 ,\n",
       "         0.01452259, -0.03068844, -0.05005021,  0.05481166, -0.0056885 ,\n",
       "        -0.03645825,  0.0230152 ,  0.03579914,  0.06651131, -0.04232946,\n",
       "        -0.03706045, -0.02657392,  0.04704131, -0.0273067 , -0.0444952 ,\n",
       "        -0.07009745, -0.08992405, -0.03695666, -0.03131334, -0.02735192,\n",
       "        -0.01821851, -0.00030886]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_vectors[144]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estalishing connection to the 'Talent referral payout' database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "mydb = mysql.connector.connect(host=\"localhost\", \n",
    "                               user=\"root\", \n",
    "                               passwd=\"root\",\n",
    "                               auth_plugin = 'mysql_native_password',\n",
    "                               database = \"talent_referral_payout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycursor = mydb.cursor(buffered = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition to obtain te referral bonus payout history of a referring professional\n",
    "\n",
    "def search_pay_db(emp_id):\n",
    "    emp_id = str(emp_id)\n",
    "    #print(type(emp_id))\n",
    "    query = \"SELECT referral_name,amount,date_of_origin FROM trp_payment_report_sample WHERE emp_num = \"+ emp_id\n",
    "    mycursor.execute(query)\n",
    "    return mycursor.fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition to obtain the status of referrals submitted by the referring professional\n",
    "\n",
    "def search_status_db(rms_id):\n",
    "    rms_id = str(rms_id)\n",
    "    #print(type(emp_id))\n",
    "    query = \"SELECT candidate_name,requisition_no,current_status,reference_date FROM trp_status WHERE candidate_id = \" + rms_id\n",
    "    mycursor.execute(query)\n",
    "    return mycursor.fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_response(t):\n",
    "    #print(query)\n",
    "    while True:\n",
    "        user_input = input(\"You:\")\n",
    "        if len(user_input) == 0:\n",
    "            print(\"Bot: Can you please provide more information\")\n",
    "        elif user_input.lower() == '_bonus_':\n",
    "            print(\"Enter your personnel number to fetch the payment details\")\n",
    "            emp_no = input(\"Emp. ID:\")\n",
    "            emp_no = str(emp_no)\n",
    "            ans = search_pay_db(emp_no)\n",
    "            #print(\"Bot: Your bonus payout details are below (in the last 6 months)\")\n",
    "            if ans is not None:\n",
    "                print(\"Bot: You were paid $\"+ str(ans[1])+\" for referring \"+str(ans[0])+\" along with \"+str(ans[2])+\" pay period\")\n",
    "                print(\"     Let me know if I can help you with anything else\")\n",
    "            else:\n",
    "                print(\"Bot: Sorry...I do not have your referral info. in my database. Please contact TRP Team\")\n",
    "        elif user_input.lower() == '_status_':\n",
    "            print(\"Bot: Enter your referral's RMS/Taleo ID to fetch the status info.\")\n",
    "            email = input(\"ID:\")\n",
    "            email = str(email)\n",
    "            ans = search_status_db(email)\n",
    "            #print(\"Bot: Below are the list of referrals you submitted in the last 6 months\")\n",
    "            if ans is not None:\n",
    "                print(\"Bot: Your referral \"+str(ans[0])+ \" has been \"+str(ans[2])+\" for requisition with ID:\"+str(ans[1]))\n",
    "                print(\"     Let me know if I can help you with anything else\")\n",
    "            else:\n",
    "                print(\"Bot: Sorry...I do not have your referral info. in my database. Please contact TRP Team\")\n",
    "        elif user_input.lower() == 'quit':\n",
    "            break\n",
    "        else:\n",
    "            #q = text_clean(user_input)\n",
    "            #q = text_preprocess(user_input)\n",
    "            q = user_input\n",
    "            q_vec = embed([q])\n",
    "            size_ = len(question_vectors)\n",
    "            sims = np.zeros(size_)\n",
    "            for i in range(size_):\n",
    "                sims[i] = cosine_similarity(q_vec, question_vectors[i])\n",
    "            max_s = sims.max()\n",
    "            if max_s < t:\n",
    "                response = \"Hmm..sorry, I don't quite understand that, can you please rephrase your question\"\n",
    "            else:\n",
    "                max_i = np.argmax(sims)\n",
    "                response = data.answer[max_i]\n",
    "            print(\"Bot:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_test():\n",
    "    print(\"Specify the confidence level\")\n",
    "    level = input(\"Confidence:\")\n",
    "    conf = int(level)/100\n",
    "    print(\"Hi There....I'm Talent Referral Bot and I'm here to help you with referral inquiries\\n[type 'quit' to stop]\")\n",
    "    print(\"***TIP: PLEASE ENTER \\n 1.'_status_' -> to check your referral status \\n 2.'_bonus_' -> to check your bonus payout status \\n 3.Else, enter your question\")\n",
    "    ans = get_response(conf)\n",
    "    #print(\"Bot:\", ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Sure, I can help you. Please type your question here\n"
     ]
    }
   ],
   "source": [
    "q = \"Can u please haaallppp\"\n",
    "q_vec = embed([q])\n",
    "#print(q_vec.shape)\n",
    "size = len(question_vectors)\n",
    "sims = np.zeros(size)\n",
    "for i in range(size):\n",
    "    sims[i] = cosine_similarity(q_vec, question_vectors[i])\n",
    "max_s = sims.max()\n",
    "if max_s < 0.4:\n",
    "    response = \"Hmm..sorry, I don't quite understand that, can you please rephrase your question\"\n",
    "else:\n",
    "    max_i = np.argmax(sims)\n",
    "    response = data.answer[max_i]\n",
    "print(\"Bot:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5857939720153809"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.89681715e-02 -6.63996860e-02 -5.76971248e-02 -6.63996860e-02\n",
      " -3.44532263e-03  5.23576029e-02 -1.27700455e-02  3.99235338e-02\n",
      " -1.08642206e-02 -1.21218869e-02  6.56280667e-04  1.59944415e-01\n",
      "  7.81605691e-02  3.05706859e-01  5.86312339e-02 -8.83430988e-02\n",
      "  4.48929481e-02  2.72031873e-04  1.33828018e-02  3.01438086e-02\n",
      " -3.34475562e-03 -6.77500963e-02  5.30438796e-02 -5.16578928e-02\n",
      " -1.09052099e-03 -2.63986476e-02 -4.80867438e-02  2.78110951e-02\n",
      "  8.31587315e-02  1.51433915e-01  1.23769633e-01 -8.20510015e-02\n",
      "  2.60227352e-01 -4.68182042e-02 -1.27102301e-01  9.35881361e-02\n",
      "  2.02220380e-01  8.49327296e-02  7.21833576e-03 -2.31734104e-02\n",
      "  2.87696905e-02  3.06047257e-02  2.83290595e-02 -1.38194449e-02\n",
      "  2.46626705e-01  1.20695613e-01  1.77700147e-01  1.28624022e-01\n",
      "  1.42477751e-01  1.87669829e-01  2.27537870e-01  1.07021615e-01\n",
      "  1.43959373e-01 -1.22436792e-01  1.01116829e-01 -1.12528041e-01\n",
      "  2.35278443e-01  1.02109000e-01  1.01292141e-01  1.00796089e-01\n",
      "  1.96581986e-02  2.03028545e-02 -4.25978824e-02  7.49953240e-02\n",
      "  7.28842989e-02  6.25611842e-02  5.28347120e-03  9.07989219e-02\n",
      "  1.62301213e-01  1.47493824e-01 -1.29590621e-02  7.07853884e-02\n",
      "  2.63075531e-01  1.49517089e-01  1.08494535e-01  1.50227979e-01\n",
      "  2.13576585e-01  5.87940123e-03  2.63369549e-03 -1.52108250e-02\n",
      "  1.05385836e-02  1.87823817e-01  4.73734923e-04  3.39945294e-02\n",
      "  3.94643173e-02 -5.62733933e-02 -3.03321593e-02 -2.59398315e-02\n",
      " -2.79222876e-02 -1.69865489e-02  4.56824526e-03 -3.50710601e-02\n",
      "  6.81325607e-03 -2.80443020e-03 -2.42557935e-02 -5.38989604e-02\n",
      " -3.15160602e-02 -5.52058779e-02  1.02708660e-01  6.19640574e-02\n",
      "  2.71242205e-02  1.53301567e-01  1.27074957e-01  1.19738251e-01\n",
      "  1.91439256e-01  6.38207495e-02  1.91439256e-01  8.53100233e-03\n",
      "  1.97173730e-02  2.54418738e-02 -8.55262578e-03 -7.39922896e-02\n",
      "  2.54418738e-02  5.16772270e-03 -4.57483120e-02  5.81771843e-02\n",
      "  9.37714800e-02  5.79980016e-02  7.54476935e-02  1.55896246e-02\n",
      " -1.75717101e-02 -7.21423980e-03 -2.22117230e-02 -1.75879654e-02\n",
      " -9.60852765e-03  1.65635720e-02  1.95217654e-02  9.31396186e-02\n",
      "  4.00737301e-02  7.12080076e-02  5.54900058e-02 -4.36121598e-02\n",
      "  1.18700042e-01  1.62829310e-02 -3.61555517e-02  8.07844251e-02\n",
      "  7.37053007e-02  2.17623889e-01 -1.00905225e-02 -3.36279534e-02\n",
      "  4.44615483e-02  1.39964625e-01  6.39060140e-02  6.75899908e-02\n",
      "  9.18968394e-02  9.39085931e-02  1.61447048e-01 -1.44968592e-02\n",
      "  1.40807275e-02  1.51707726e-02 -5.17587252e-02 -6.11240156e-02\n",
      "  7.75806308e-02  2.77887844e-03  7.51113333e-03  7.11031854e-02\n",
      "  6.72796071e-02  1.51934966e-01 -5.27557917e-03  2.71307305e-02\n",
      "  6.56833127e-02  1.57918602e-01  9.71043259e-02  1.50430590e-01\n",
      "  8.26931559e-03  2.21629534e-03  5.96881211e-02  3.60058695e-02\n",
      " -6.27057627e-04  2.06225574e-01  8.15295652e-02  9.20053422e-02\n",
      "  8.73342343e-03 -2.30363533e-02  8.58098455e-03  3.51264514e-02\n",
      " -4.29131351e-02  1.90457515e-02  4.66316976e-02 -6.12474829e-02\n",
      "  3.12508345e-02 -1.08126923e-02 -7.39916600e-03  9.17295739e-02\n",
      " -1.29509894e-02 -3.98115292e-02  6.09700680e-02  3.07008140e-02\n",
      "  6.00921325e-02  6.54070601e-02 -1.28858238e-02  2.30400283e-02\n",
      "  2.59837694e-02 -6.60660490e-02 -1.36661045e-02 -2.47465074e-02\n",
      " -9.73783731e-02  1.20305300e-01  1.28188282e-01  1.77700147e-01\n",
      "  3.58620472e-02  1.38332797e-02  5.35188578e-02  8.76323208e-02\n",
      "  3.88122946e-02  9.71917212e-02  8.15556496e-02  1.16410330e-02\n",
      " -8.50516278e-03  1.46249272e-02  2.12204643e-02 -1.31293600e-02\n",
      " -2.74222586e-02 -2.62462310e-02 -6.10771030e-02  8.39079171e-02\n",
      "  9.48897153e-02  1.09854110e-01  1.19734719e-01  2.13704497e-01\n",
      "  6.59660697e-02 -1.25369439e-02  1.12127334e-01 -5.38822040e-02\n",
      "  2.15329658e-02  7.07837641e-02  1.15580656e-01  1.34355843e-01\n",
      "  6.56897277e-02  1.35639943e-02  3.52814868e-02  5.53271994e-02\n",
      "  4.66426164e-02  1.93114981e-01  2.39829615e-01  8.40526000e-02\n",
      "  5.64362481e-02  1.81456298e-01  1.80745155e-01  1.96745366e-01\n",
      "  5.06357923e-02  1.25055630e-02  1.01477079e-01  9.49485376e-02\n",
      " -1.46698551e-02 -8.68610740e-02  4.57382724e-02 -2.95037366e-02\n",
      " -1.08456902e-01  1.99899599e-02 -2.52316706e-02  5.32052740e-02\n",
      "  6.71018884e-02  6.74191257e-03  2.02387795e-01  1.18631236e-01\n",
      "  1.15479261e-01  1.32541567e-01  6.26080111e-02  2.13453472e-01\n",
      "  1.57045633e-01  4.34534028e-02  3.44433561e-02 -1.97600294e-02\n",
      "  1.22685470e-01  2.21873850e-01  4.26934175e-02  1.59251869e-01\n",
      "  1.16662599e-01  1.43788040e-01  1.24802858e-01  9.72431228e-02\n",
      "  7.40625113e-02  4.15921807e-02 -4.38849181e-02  2.83379555e-02\n",
      "  2.81133242e-02  1.67765141e-01  2.10849106e-01  1.36427984e-01\n",
      "  8.01963285e-02  1.89009041e-01  2.39224657e-02  6.95917010e-02\n",
      "  9.73408669e-02 -2.66929753e-02  6.33203238e-02  5.99632226e-03\n",
      "  4.78692986e-02 -1.26584712e-03 -2.16164198e-02 -5.39103709e-03\n",
      "  1.85961630e-02 -4.88586873e-02 -3.67118157e-02 -7.80485123e-02\n",
      " -2.65280046e-02  8.37804899e-02 -2.65280046e-02  9.69855115e-04\n",
      "  3.44171338e-02  3.40675414e-02 -4.51816898e-03  8.12136233e-02\n",
      " -9.47114974e-02 -9.36602354e-02 -4.45675254e-02 -9.36602354e-02\n",
      " -1.86036006e-01  7.79746100e-02  5.88985719e-02 -4.87649553e-02\n",
      " -1.30676115e-02 -4.18352038e-02 -7.16419592e-02  1.40968151e-03\n",
      "  1.55168682e-01  2.60888878e-03 -1.20589823e-01 -2.88735293e-02\n",
      " -7.15080574e-02 -9.35051404e-03 -6.53563589e-02 -9.34002399e-02\n",
      "  1.04003213e-01 -4.90090586e-02  1.60231441e-03 -1.33271068e-02\n",
      " -1.95751414e-02  3.52871977e-02  3.07080299e-01  1.60188943e-01\n",
      "  1.73161834e-01 -5.37017062e-02 -3.18658352e-02 -3.62679623e-02\n",
      " -7.49672353e-02  1.88531633e-02 -3.14820968e-02  1.48558915e-01\n",
      "  1.53249502e-01  6.63766935e-02 -7.08638430e-02 -4.12069112e-02\n",
      "  8.35710615e-02 -6.19627610e-02 -2.54043825e-02 -7.71211237e-02\n",
      " -1.80588290e-02  6.37630373e-02  1.22510515e-01  5.11243939e-03\n",
      "  1.78559989e-01  9.07027349e-02  9.35895219e-02  6.47145510e-02\n",
      "  3.31305675e-02  1.23282813e-01  7.47898370e-02  7.47898370e-02\n",
      "  1.11423898e-02  1.05120115e-01  9.28432569e-02 -1.38299912e-02\n",
      "  1.01871580e-01 -4.79210168e-04  1.33413136e-01  8.06801021e-02\n",
      "  8.54013339e-02  9.58932564e-02  7.31067359e-02  1.01686761e-01\n",
      " -2.83247530e-02 -4.00483869e-02  7.49859214e-02 -5.15004322e-02\n",
      "  4.37710807e-03 -5.56557104e-02  3.40639129e-02 -6.41302839e-02\n",
      "  2.79644847e-01  8.35736394e-02  2.07993865e-01  1.49036244e-01\n",
      "  1.31052941e-01  1.02822110e-01 -1.52838230e-03  8.34036618e-02\n",
      "  1.49638042e-01  1.30088225e-01  1.46491945e-01  1.15632504e-01\n",
      "  9.17307064e-02  2.55442932e-02  9.36067030e-02  1.30239770e-01\n",
      "  1.31697446e-01  9.99715030e-02  1.40849948e-01  9.08674598e-02\n",
      "  1.65574238e-01  1.66955367e-01  2.11060897e-01  2.05122799e-01\n",
      "  3.17164101e-02  5.65647930e-02  3.52978185e-02  4.86012213e-02\n",
      "  8.40391070e-02  8.06954876e-02  9.43902433e-02  1.49386555e-01\n",
      "  2.58842766e-01  2.19788685e-01  1.72347814e-01  1.72738642e-01\n",
      "  4.41343933e-02  4.49972861e-02  1.45726493e-02  7.57875592e-02\n",
      "  6.92019016e-02  8.71334821e-02  4.26140055e-02  1.05983421e-01\n",
      "  1.16518371e-01  1.24561965e-01  4.07078564e-02  1.39342353e-01\n",
      "  6.06809482e-02  1.52115598e-01  1.60034150e-01  2.11155534e-01\n",
      "  4.60661471e-01  2.84897059e-01  2.77532935e-01  9.96434242e-02\n",
      "  1.89137757e-01  1.53080240e-01  1.91257328e-01  1.97636113e-01\n",
      "  1.83208987e-01  2.97710359e-01  3.24779868e-01  1.99997798e-01\n",
      "  2.05311790e-01  2.89611757e-01  1.85128957e-01  2.49163896e-01\n",
      "  2.18353942e-01  9.52151641e-02  5.85793972e-01  7.36111701e-02\n",
      "  7.99845457e-02  8.73617530e-02  1.46281749e-01  5.89083061e-02\n",
      "  6.39201626e-02  1.53389871e-01  1.03153601e-01  1.24794684e-01\n",
      "  8.65839049e-02  2.17817336e-01  1.63665980e-01  1.60643324e-01\n",
      "  1.54019088e-01  1.27935767e-01  1.37173116e-01  1.33652776e-01\n",
      "  7.50813782e-02  6.63743466e-02  5.04126735e-02  1.21427573e-01\n",
      "  8.78768116e-02  6.67590350e-02 -9.23657417e-03  9.35199261e-02\n",
      "  7.10381567e-02  3.49867716e-02  1.22101426e-01  1.49585873e-01\n",
      "  1.44605666e-01  5.13117090e-02  6.00121077e-03 -8.87236521e-02\n",
      "  4.78633717e-02  2.47237720e-02  9.08254981e-02  5.58109321e-02\n",
      "  1.86328702e-02  1.25110485e-02 -3.53683252e-04 -1.05692111e-02\n",
      " -2.44799741e-02 -1.28196850e-02  6.07706420e-02  6.66342676e-02\n",
      "  1.25002265e-02  1.15686521e-01  6.04530126e-02  2.53939498e-02\n",
      "  8.07870626e-02  2.99029890e-02  4.16384265e-02  5.83681762e-02\n",
      " -3.31361517e-02 -1.07123256e-01 -7.53105581e-02  1.98857382e-01\n",
      " -2.06512548e-02  1.02772824e-02  6.87527284e-02  9.67848897e-02\n",
      "  2.80248281e-03  3.30852680e-02  1.60411373e-02  1.02772824e-02\n",
      "  7.03986809e-02  8.89636129e-02  5.20597771e-02  7.57786557e-02\n",
      "  8.86609107e-02  5.52504137e-02  8.80633071e-02  7.23582953e-02\n",
      "  1.01023212e-01  1.47987157e-01  1.77311912e-01  1.08663831e-02\n",
      "  2.81099025e-02  4.00896296e-02 -2.09512860e-02 -2.49489658e-02\n",
      "  2.35925958e-01  1.22393146e-01 -3.07830796e-02  3.66895199e-02\n",
      "  1.18153647e-01 -6.45338371e-03 -7.87002221e-02 -1.13481265e-02\n",
      "  4.38046753e-02 -1.50992842e-02 -3.35497931e-02 -4.82511744e-02\n",
      " -3.52603234e-02 -1.04775518e-01 -5.94591796e-02 -4.45261374e-02\n",
      " -8.00472423e-02 -4.24079858e-02 -4.40337285e-02 -2.63910834e-02\n",
      "  7.69998431e-02 -6.43820092e-02  1.60419829e-02 -1.28856659e-01\n",
      "  5.19784726e-02 -2.31847316e-02 -8.92098919e-02 -4.98351380e-02\n",
      "  1.16388917e-01  1.16388917e-01 -9.28331167e-02 -4.43420652e-03\n",
      "  5.57031408e-02 -5.22981733e-02  1.81528665e-02  1.13727637e-01\n",
      " -4.24478799e-02  4.18452322e-02  7.03899562e-02  7.53622800e-02\n",
      "  1.27897710e-01  2.27165073e-01  3.49397287e-02  5.29200882e-02\n",
      "  5.43036498e-02 -2.45594215e-02  1.65355876e-01  5.22283912e-02\n",
      "  6.66459575e-02  6.80590123e-02 -1.45333754e-02  8.87573659e-02\n",
      " -2.36485545e-02  1.77017078e-02  1.69762690e-02  8.99957120e-02\n",
      "  2.35422909e-01  9.64381397e-02  9.05047953e-02  6.98979869e-02\n",
      "  2.75927205e-02 -5.10024652e-03  1.17589990e-02  3.19372900e-02\n",
      "  2.50951350e-02  1.45795792e-02 -6.57497048e-02  1.12471849e-01\n",
      " -2.20475569e-02 -3.37822363e-02 -1.04118995e-02  3.53103019e-02\n",
      " -7.68702477e-02  8.08096081e-02  1.02898620e-01 -3.74534540e-02\n",
      "  8.62510651e-02 -7.81612843e-02 -1.73677132e-02 -4.02215719e-02\n",
      "  1.46587966e-02  3.61528918e-02  9.99028087e-02  5.28600886e-02\n",
      "  2.02440843e-03 -1.55952461e-02  9.19282585e-02  5.20583130e-02\n",
      " -4.80671972e-03  3.31469476e-02  2.54718624e-02  2.65627168e-02\n",
      " -2.31629256e-02  4.67322655e-02  5.06884307e-02  7.58337379e-02\n",
      " -2.91713141e-03 -3.30395252e-03  3.83851565e-02 -7.05126300e-03\n",
      "  4.52233478e-03  4.10685390e-02  2.48638559e-02 -5.35743833e-02\n",
      "  1.07042685e-01 -2.50456911e-02 -4.69679013e-02 -1.83530077e-02\n",
      "  7.31919855e-02  9.96968895e-02  4.70145941e-02 -1.28661841e-01\n",
      " -3.72223333e-02  9.34887957e-03 -4.65381294e-02  3.84373963e-02\n",
      " -7.03621358e-02 -2.61242967e-02 -3.24078724e-02 -8.20311531e-02\n",
      "  5.64831868e-02  6.01791441e-02  1.01347327e-01  6.96704313e-02\n",
      "  8.75957683e-03  6.83577061e-02  6.92503303e-02  6.73130155e-02\n",
      "  1.29772335e-01  6.83994740e-02  4.27152812e-02 -9.98897851e-03\n",
      " -3.69684547e-02  1.06753647e-01  4.34249081e-03  1.36312410e-01\n",
      "  1.57810152e-02 -1.23542156e-02  4.97150868e-02 -5.41005395e-02\n",
      " -6.08220920e-02  8.07869807e-03  6.32168651e-02  2.02480592e-02\n",
      "  2.27172561e-02 -3.69189605e-02  5.82974739e-02  2.83202566e-02\n",
      " -9.32137072e-02 -3.40104587e-02  5.52864373e-03  1.82045717e-02\n",
      " -1.47057511e-02  2.12069452e-02 -3.19052748e-02  9.37786475e-02\n",
      "  7.46630877e-02  6.54631406e-02  1.18446052e-02  1.54768238e-02\n",
      "  4.18138318e-03  5.90387881e-02  6.21393286e-02  8.64156708e-03\n",
      " -4.39819060e-02 -3.64230461e-02  1.08679734e-01 -7.15436786e-02\n",
      " -2.35232757e-03  1.37540504e-01  7.17242286e-02  1.08418956e-01\n",
      " -6.63996860e-02  1.65936142e-01  7.96644539e-02  7.26494193e-02\n",
      "  1.40246347e-01  2.27439702e-01 -2.70035826e-02  5.41285947e-02\n",
      "  7.67969042e-02  9.13366526e-02  1.91836178e-01 -2.47546379e-02\n",
      "  6.55423701e-02  3.61337997e-02  6.68071806e-02 -1.22356396e-02\n",
      " -3.25047821e-02  1.62311852e-01  2.75695741e-01  1.86708748e-01\n",
      "  7.38903694e-03  4.54618335e-02  4.02275324e-02  2.31413350e-01\n",
      "  1.13107771e-01 -1.52646415e-02 -5.83801791e-02  1.00990824e-01\n",
      "  7.14157894e-03 -1.81540772e-02 -8.07363540e-02 -8.22741166e-02\n",
      " -1.84046142e-02  1.74216926e-02  2.19209436e-02  9.02941152e-02\n",
      " -5.01126330e-03 -2.05726624e-02  2.19209436e-02  3.10714021e-02\n",
      "  4.53744903e-02  1.87188119e-01  1.09581791e-01  1.49234058e-02\n",
      " -1.13539584e-02  3.78168784e-02  1.78029895e-01 -3.19913328e-02\n",
      " -4.80772480e-02 -2.40181088e-02 -8.88287835e-03 -1.27981678e-02\n",
      "  6.63025901e-02  9.04254392e-02 -4.60465252e-02  1.82569586e-02\n",
      "  4.31370959e-02 -3.09596900e-02 -4.61640209e-02 -9.78623554e-02\n",
      "  1.43976947e-02 -5.43095246e-02  3.17678154e-02  1.19179182e-01\n",
      "  4.67624217e-02 -1.54568451e-02  2.93608811e-02  3.08966432e-02\n",
      " -2.73972508e-02 -7.93547742e-03  2.25274172e-02  1.12807229e-01\n",
      " -1.54568451e-02 -6.01710901e-02 -1.41228348e-01  6.52246848e-02\n",
      "  5.48233986e-02  5.97139113e-02 -7.15097710e-02  2.09784985e-01\n",
      "  1.35258958e-01  1.48139328e-01  1.04366973e-01  5.85166961e-02\n",
      "  6.82143867e-02  1.00721791e-01  1.00721791e-01  1.58702582e-01\n",
      "  2.98339099e-01  3.54850553e-02 -1.96026824e-03  3.05882394e-02\n",
      "  5.06695285e-02 -7.52559118e-03 -1.17513109e-02  8.40568766e-02\n",
      "  2.88398061e-02  1.00832187e-01  1.77294940e-01  7.55074620e-02\n",
      "  1.10483147e-01 -4.28500995e-02  7.88449422e-02  2.66556323e-01\n",
      "  1.93159953e-01  1.84504494e-01  1.71942413e-02 -3.87130231e-02\n",
      "  1.08331159e-01  1.14739597e-01  1.45736821e-02  1.87844962e-01\n",
      "  2.30120957e-01  7.26649910e-03  1.62416369e-01  5.32659292e-02\n",
      " -2.93147862e-02 -6.13641739e-02  1.95752889e-01 -2.27477103e-02\n",
      " -6.84769824e-02 -6.89019039e-02  1.99305311e-01 -7.69385695e-03\n",
      "  3.72898858e-03 -4.20455076e-03  9.64165926e-02  1.44860763e-02\n",
      "  8.56284797e-02 -1.90739892e-02 -3.93618904e-02 -1.50839463e-02\n",
      "  5.82410544e-02  8.83949548e-02 -4.90431488e-03  7.97652528e-02\n",
      "  7.08725154e-02  8.27427134e-02 -1.84284672e-02  7.79664218e-02\n",
      "  4.76631522e-02  2.07849555e-02 -9.47734267e-02  2.73969695e-02\n",
      " -2.64596250e-02 -2.27594040e-02  8.33355337e-02  1.08241454e-01\n",
      "  2.21803024e-01  8.91244486e-02 -2.47872137e-02 -2.96516865e-02\n",
      " -5.45880944e-03  1.18923649e-01  7.69084841e-02  1.06172293e-01\n",
      "  1.68358851e-02  7.34651685e-02  1.80593461e-01  1.35626033e-01\n",
      " -3.62240151e-02  1.06940918e-01 -3.21606360e-02 -5.17097786e-02\n",
      "  6.14193715e-02  3.67117226e-02  1.48020118e-01  9.66131501e-03\n",
      "  1.53444195e-02  1.60228387e-01 -3.54719162e-03  1.23443520e-02\n",
      "  1.46704301e-01  7.62623772e-02  4.74150702e-02 -4.95416597e-02\n",
      "  9.53118131e-02  6.96286410e-02  1.33801311e-01  1.35484010e-01\n",
      "  1.65342450e-01  3.20584048e-03 -2.80244090e-02  3.99168953e-02\n",
      "  2.04723291e-02  1.46601617e-01 -7.90360868e-02 -3.22884880e-02\n",
      "  1.64429657e-02  8.84827226e-02  6.70675710e-02  7.30701387e-02\n",
      " -1.30248982e-02  7.02596530e-02  9.42861438e-02  4.40541096e-02\n",
      " -2.38742493e-02  1.54728051e-02 -1.37127489e-02 -1.61131881e-02\n",
      "  6.42791837e-02  9.93099250e-03 -8.95554200e-02 -9.34278518e-02\n",
      " -2.41407044e-02 -7.18778670e-02  8.59237909e-02  1.02305308e-01\n",
      "  2.52097100e-02  1.05825692e-01  2.01364364e-02  8.47864449e-02\n",
      "  2.29290985e-02  1.31251499e-01  1.37997389e-01  2.87039101e-01\n",
      " -3.32832746e-02  1.17589056e-01  1.37997389e-01  1.16633877e-01\n",
      "  6.32585436e-02 -5.22357747e-02 -2.90629342e-02  1.07068390e-01\n",
      "  6.91500530e-02  2.31155567e-03  1.44076198e-01  2.34179065e-01\n",
      "  9.08180177e-02  4.62790430e-02  1.07514396e-01  3.56706418e-02\n",
      "  1.85770225e-02  4.88865189e-04 -1.01000005e-02  9.55292210e-03\n",
      "  6.82978481e-02  9.96534452e-02 -5.42296395e-02  6.54963851e-02\n",
      " -2.74928659e-02  9.07122791e-02  1.52583748e-01  4.86027747e-02\n",
      "  7.54399449e-02  7.68974051e-02  1.18328959e-01  1.38007000e-01\n",
      "  3.52060795e-02  5.84293753e-02  8.83674771e-02  1.87923964e-02\n",
      "  9.32910740e-02  1.00715563e-01  1.54657781e-01  7.44772255e-02\n",
      "  1.18339464e-01  2.70834900e-02  2.36195102e-01  1.85116127e-01\n",
      "  2.57939864e-02  7.44772255e-02  3.57242823e-02  1.50950849e-01\n",
      "  1.21563569e-01  1.70161173e-01 -6.33834302e-03  8.98627788e-02\n",
      "  7.29976445e-02  3.67889330e-02  1.39675930e-01  9.72302705e-02\n",
      "  1.96711555e-01  7.54420906e-02  1.29114583e-01  2.44275838e-01\n",
      "  8.31075609e-02  8.43586102e-02]\n"
     ]
    }
   ],
   "source": [
    "size = len(question_vectors)\n",
    "sims = np.zeros(size)\n",
    "for i in range(size):\n",
    "    sims[i] = cosine_similarity(q_vec, question_vectors[i])\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n213408\\n239660\\n249058\\n250194\\n260943\\n271637\\n274346\\n282822\\n284183\\n284315\\n284754\\n285695\\n290560\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sample RMS IDs for chatbot testing of referral status\n",
    "\"\"\"\n",
    "24211419\n",
    "18327009\n",
    "24482992\n",
    "21996054\n",
    "22425736\n",
    "24426987\n",
    "17783534\n",
    "24601097\n",
    "23736632\n",
    "24698456\n",
    "24623156\n",
    "21705392\n",
    "\"\"\"\n",
    "#Sample Employee IDs for chatbot testing bonus status\n",
    "\"\"\"\n",
    "213408\n",
    "239660\n",
    "249058\n",
    "250194\n",
    "260943\n",
    "271637\n",
    "274346\n",
    "282822\n",
    "284183\n",
    "284315\n",
    "284754\n",
    "285695\n",
    "290560\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of TF USE Transformer \n",
      " 0.6\n",
      "Average Simailarity score 0.7118765065649978\n"
     ]
    }
   ],
   "source": [
    "#Testing the Chabot with Test Dataset\n",
    "os.chdir('C:\\\\Users\\\\akadali\\\\Desktop\\\\Deep_NLP\\\\MLG_Capstone_ChatBot\\\\ChatBot_GoogleW2V')\n",
    "\n",
    "#importing test dataset\n",
    "test_data = pd.read_csv(\"chatbot_test.csv\", encoding = 'latin1')\n",
    "#test_data.drop(['Unnamed: 3'], axis = 1, inplace = True)\n",
    "size = len(question_vectors)\n",
    "sims = np.zeros(size)\n",
    "def test_response(q, vecs = question_vectors):\n",
    "    q = text_clean(q)\n",
    "    q = text_preprocess(q)\n",
    "    q_vec = embed([q])\n",
    "    for i in range(size):\n",
    "        sims[i] = cosine_similarity(q_vec, question_vectors[i])\n",
    "    max_s = sims.max()\n",
    "    max_i = np.argmax(sims)\n",
    "    return max_s, data.answer[max_i], data.question[max_i], data.intent[max_i]\n",
    "\n",
    "test_data['bot_response'] = test_data['test_question'].apply(lambda x:test_response(x)[1])\n",
    "test_data['train_question'] = test_data['test_question'].apply(lambda x:test_response(x)[2])\n",
    "test_data['sim_score'] = test_data['test_question'].apply(lambda x:test_response(x)[0])\n",
    "test_data['train_intent'] = test_data['test_question'].apply(lambda x:test_response(x)[3])\n",
    "\n",
    "test_data['valid'] = test_data['bot_response'] == test_data['test_answer']\n",
    "\n",
    "print(\"Accuracy of TF USE Transformer \\n\",(test_data.valid.sum()/len(test_data)).round(2))\n",
    "print(\"Average Simailarity score\",test_data.sim_score.mean())\n",
    "\n",
    "##First Test result without stop words, with lemmatization\n",
    "# Accuracy - 55%\n",
    "# Mean Similarity score - 0.70\n",
    "\n",
    "##Second Test Result - without stop words, with lemmatization and \n",
    "# Accuracy - 61%\n",
    "# Mean Similarity score - 0.71\n",
    "\n",
    "##Third Test Result - without stop words, without lemmatization\n",
    "# Accuracy - 60\n",
    "# Mean Similarity score - 0.71\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv(\"C:\\\\Users\\\\akadali\\\\Desktop\\\\Deep_NLP\\\\MLG_Capstone_ChatBot\\\\ChatBot_GoogleW2V\\\\test_results_tran.csv\",\n",
    "                 index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-TF2.0",
   "language": "python",
   "name": "py3-tf2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
